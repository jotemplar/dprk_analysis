# DPRK Research and Analysis System

A comprehensive system for searching, capturing, and analyzing both **images** and **text content** related to North Korean workers and military personnel in Russia, designed for human rights documentation and research purposes.

**Latest Update (v3.2.0)**: Major codebase reorganization with clean folder structure. All files moved to logical directories (reports/, search_terms/, tests/, scripts/), updated import paths, and consolidated root directory from 132 files to 15 core files. Enhanced reporting with comprehensive Excel exports including search term attribution, interactive HTML dashboards combining article and image analysis, and advanced entity consolidation with government/corporate deduplication.

## ðŸš€ Quick Command Reference

```bash
# First-time setup (one command)
./setup.sh

# Daily operations
./run.sh test    # Test with 2 queries (~2 min)
./run.sh full    # Full pipeline (~30-60 min)
./run.sh basic   # System health check

# Enhanced Analysis Pipeline (Image Processing)
python scripts/image/process_missing_llava_parallel.py     # Process images missing LLaVA analysis
python scripts/image/process_all_gemma12b_parallel.py      # Process all images with Gemma3:12b
python scripts/image/apply_ensemble_analysis.py            # Apply ensemble combining both models
python scripts/reporting/export_to_spreadsheet.py          # Export comprehensive analysis results

# Text Article Processing Pipeline
python main_article_pipeline.py --status     # Check current article processing status
python main_article_pipeline.py --full       # Run complete article pipeline (searchâ†’scrapeâ†’analyze)
python scripts/article/process_article_searches.py         # Execute search terms from pack 3 (58 queries)
python scripts/article/process_article_content.py          # Scrape article content (50 concurrent scrapers)
python scripts/article/process_article_analysis.py         # Analyze articles with Gemma3:12b

# Russian OSINT Pipeline
./run_russian_pipeline.sh search            # Process Russian search queries (Yandex + Google Russia)
./run_russian_pipeline.sh export            # Export results to Excel
./run_russian_pipeline.sh report            # Generate HTML report
./run_russian_pipeline.sh full              # Run complete Russian OSINT pipeline
./run_russian_pipeline.sh status            # Check pipeline status

# Reporting and Export
python scripts/reporting/generate_article_report.py        # Generate HTML/JSON analysis reports
python scripts/reporting/export_articles_to_excel.py       # Export articles with search term attribution
python scripts/dashboard/serve_dashboard.py                # Launch interactive dashboard server

# Key maintenance
uv run --no-project python init_database.py  # Reset database
ollama serve                                  # Start LLM service
psql -U postgres -d dprk                      # Database access
du -sh captured_data/                         # Check storage
```

## Overview

This system automates the process of:
- **Image Pipeline**: Searching, capturing, downloading, and analyzing images using multilingual search terms
- **Text Pipeline**: Searching for articles, scraping content, and analyzing text using AI models
- Capturing screenshots of search results for visual context
- Downloading and storing images with comprehensive metadata
- Analyzing both images and text using local LLM for sensitive content
- Storing all results in a PostgreSQL database for research and reporting

## Features

### Image Analysis Pipeline
- **Multilingual Search**: Supports 104 image search terms in English, Russian, Korean, Chinese, and French
- **Themed Search Terms**: Organized by exploitation themes (construction, dorms, handlers, community, financial)
- **Image Capture**: Downloads images and captures screenshots of search results
- **Ensemble AI Analysis**: Combines multiple models (LLaVA + Gemma3:12b) for higher accuracy
- **Parallel Processing**: 4 concurrent AI analysis threads for faster processing
- **Image Standardization**: Automatic resizing to 896x896 for optimal model performance
- **Metadata Extraction**: Extracts EXIF data, GPS coordinates, and other metadata

### Text Article Pipeline (New!)
- **Article Search**: 58 specialized text search queries across 7 categories
- **High-Performance Scraping**: 50 concurrent Firecrawl scrapers for optimal throughput
- **Content Extraction**: Markdown and HTML content with metadata
- **AI Text Analysis**: Gemma3:12b analysis for DPRK relevance and concern levels
- **Multilingual Support**: English, Russian, Korean, and Chinese text processing
- **Site-Specific Targeting**: VK, Telegram, forums, and government sites

### Shared Infrastructure
- **Humanitarian Focus**: Specialized prompts for detecting exploitation and poor conditions
- **Local Analysis**: Uses Ollama for sensitive content analysis to avoid cloud filtering
- **Database Storage**: PostgreSQL database with comprehensive schema for all data
- **Batch Processing**: Concurrent downloads and efficient pipeline processing
- **Privacy-Focused**: Local LLM analysis for sensitive content
- **Comprehensive Export**: Excel spreadsheets with multiple analysis sheets

## Installation

### Prerequisites

- Python 3.12+
- PostgreSQL 14+
- Ollama (for local LLM)
- Chrome/Chromium (for Playwright)

### Setup

1. **Clone the repository**:
```bash
cd /Volumes/X5/_CODE_PROJECTS/DPRK
```

2. **Install Python dependencies**:
```bash
uv pip install -r requirements.txt
uv run playwright install chromium
```

3. **Set up Ollama**:
```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Start Ollama service
ollama serve

# Pull required models
ollama pull llava
ollama pull gemma3:12b     # For ensemble analysis
ollama pull gemma3n:e4b    # Optional, for comparison
```

4. **Configure environment**:
- Copy `.env.example` to `.env` (already done)
- Update API keys and database credentials in `.env`

5. **Initialize database**:
```bash
python init_database.py
```

## Quick Start Commands

### Essential Setup (One-time)
```bash
# Install uv package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# Run automated setup (creates venv, installs deps, initializes DB)
./setup.sh

# Or manual setup:
uv venv                                  # Create virtual environment
uv pip install -r requirements.txt      # Install Python dependencies
uv run playwright install chromium       # Install browser for screenshots
uv run --no-project python init_database.py  # Initialize database
```

### Daily Usage Commands
```bash
# Using the convenient run script:
./run.sh basic   # Test system components
./run.sh test    # Run pipeline with 2 test queries (~2 min)
./run.sh full    # Run full pipeline - ALL queries (~30-60 min)
./run.sh init    # Reset/reinitialize database

# Direct commands with uv:
uv run --no-project python test_basic.py     # System health check
uv run --no-project python main.py test      # Test mode (2 queries)
uv run --no-project python main.py           # Full pipeline
```

### Database Commands
```bash
# Initialize/reset database with search terms
uv run --no-project python init_database.py

# PostgreSQL direct access
psql -U postgres -d dprk                     # Connect to database
psql -U postgres -d dprk -c "SELECT COUNT(*) FROM search_queries;"
psql -U postgres -d dprk -c "SELECT COUNT(*) FROM captured_images;"

# Backup database
pg_dump -U postgres dprk > dprk_backup_$(date +%Y%m%d).sql

# Restore database
psql -U postgres -d dprk < dprk_backup_20250119.sql
```

### Ollama Commands (Local LLM)
```bash
# Start Ollama service
ollama serve

# Check Ollama status
curl http://localhost:11434/api/tags

# Pull required model (if not installed)
ollama pull llava

# List installed models
ollama list
```

### Monitoring & Maintenance
```bash
# Check system status
./run.sh basic

# View captured images
ls -la captured_data/images/$(date +%Y-%m-%d)/

# View screenshots
ls -la captured_data/screenshots/$(date +%Y-%m-%d)/

# Check logs
tail -f logs/pipeline_$(date +%Y%m%d).log

# Disk usage
du -sh captured_data/

# Clean old data (30+ days)
find captured_data/ -type f -mtime +30 -delete
```

## Project Structure

```
DPRK/
â”œâ”€â”€ reports/              # All generated reports (HTML, XLSX, JSON)
â”œâ”€â”€ search_terms/         # All search term definitions and CSV files
â”‚   â”œâ”€â”€ dprk_images_search_terms.py       # Image search terms (pack 1)
â”‚   â”œâ”€â”€ dprk_images_search_terms_2.py     # Image search terms (pack 2)
â”‚   â”œâ”€â”€ dprk_images_search_terms_3.py     # Article search terms (pack 3)
â”‚   â”œâ”€â”€ dprk_images_search_terms_combined.py  # Combined image search terms
â”‚   â””â”€â”€ dprk_osint_queries_with_social_and_portals_v1_3.csv  # Russian OSINT queries
â”œâ”€â”€ tests/                # All test scripts
â”‚   â””â”€â”€ test_basic.py
â”œâ”€â”€ scripts/              # Organized processing scripts
â”‚   â”œâ”€â”€ image/           # Image pipeline scripts
â”‚   â”‚   â”œâ”€â”€ process_missing_llava_parallel.py
â”‚   â”‚   â”œâ”€â”€ process_all_gemma12b_parallel.py
â”‚   â”‚   â””â”€â”€ apply_ensemble_analysis.py
â”‚   â”œâ”€â”€ article/         # Article pipeline scripts
â”‚   â”‚   â”œâ”€â”€ process_article_searches.py
â”‚   â”‚   â”œâ”€â”€ process_article_content.py
â”‚   â”‚   â””â”€â”€ process_article_analysis.py
â”‚   â”œâ”€â”€ russian/         # Russian OSINT scripts
â”‚   â”‚   â””â”€â”€ process_russian_searches.py
â”‚   â”œâ”€â”€ reporting/       # Export and reporting scripts
â”‚   â”‚   â”œâ”€â”€ export_to_spreadsheet.py
â”‚   â”‚   â”œâ”€â”€ export_articles_to_excel.py
â”‚   â”‚   â”œâ”€â”€ export_russian_searches_to_excel.py
â”‚   â”‚   â”œâ”€â”€ generate_article_report.py
â”‚   â”‚   â””â”€â”€ generate_russian_search_report.py
â”‚   â””â”€â”€ dashboard/       # Dashboard generation scripts
â”‚       â””â”€â”€ serve_dashboard.py
â”œâ”€â”€ logs/                 # All log files
â”œâ”€â”€ docs/                 # Documentation files
â”œâ”€â”€ capture/              # Screenshot and image capture modules
â”‚   â”œâ”€â”€ screenshot_capture.py
â”‚   â””â”€â”€ image_downloader.py
â”œâ”€â”€ database/             # Database models and connection
â”‚   â”œâ”€â”€ models.py         # Image analysis database models
â”‚   â”œâ”€â”€ article_models.py # Article analysis database models
â”‚   â”œâ”€â”€ russian_search_models.py  # Russian OSINT database models
â”‚   â””â”€â”€ connection.py
â”œâ”€â”€ search/               # Search API clients
â”‚   â”œâ”€â”€ serp_image_client.py   # Image search client
â”‚   â”œâ”€â”€ serp_web_client.py     # Web/article search client
â”‚   â””â”€â”€ serp_russia_client.py  # Russian search client (Yandex/Google Russia)
â”œâ”€â”€ utils/                # Utility modules
â”‚   â””â”€â”€ ollama_analyzer.py
â”œâ”€â”€ captured_data/        # Storage for captured content
â”‚   â”œâ”€â”€ images/
â”‚   â””â”€â”€ screenshots/
â”œâ”€â”€ main_article_pipeline.py       # Article pipeline orchestrator
â”œâ”€â”€ init_database.py      # Database initialization
â”œâ”€â”€ main.py              # Main image pipeline
â”œâ”€â”€ main_with_dedup.py   # Image pipeline with deduplication
â”œâ”€â”€ run.sh               # Convenience script for image pipeline
â”œâ”€â”€ run_russian_pipeline.sh  # Convenience script for Russian OSINT pipeline
â”œâ”€â”€ .env                 # Configuration (not in git)
â”œâ”€â”€ README.md            # This file
â””â”€â”€ CHANGELOG.md         # Version history
```

## Database Schema

### Image Analysis Tables
- `search_queries`: Search terms and categories
- `search_results`: URLs from image searches
- `captured_images`: Downloaded image metadata
- `screenshots`: Screenshot metadata
- `content_analysis`: LLM analysis results for images
- `search_sessions`: Session tracking

### Article Analysis Tables (New!)
- `article_searches`: Article search terms and metadata
- `article_results`: URLs from article searches
- `article_content`: Scraped article content (markdown/HTML)
- `article_analysis`: AI analysis results for articles

## API Services

- **SERP API**: Google image and web search
- **Firecrawl**: High-performance web scraping (50 concurrent scrapers)
- **Jina AI**: Backup content extraction
- **Ollama**: Local LLM for analysis (LLaVA for images, Gemma3:12b for text)

## Configuration

### Environment Variables
- `DB_NAME`: PostgreSQL database name (default: dprk)
- `OLLAMA_HOST`: Ollama server URL
- `OLLAMA_MODEL`: Model for analysis (default: llava)
- `IMAGE_STORAGE_PATH`: Path for image storage
- `SCREENSHOT_STORAGE_PATH`: Path for screenshots
- `SEARCH_RATE_LIMIT`: API rate limit
- `CONCURRENT_SCRAPERS`: Parallel download threads
- `FIRECRAWL_API_KEY`: Firecrawl API key for article scraping

## Search Categories

### Image Search Categories (Packs 1 & 2)
1. **Region**: Far East, Kursk, Vladivostok, Siberia
2. **Labour Type**: Construction, mining, industrial
3. **Military**: Soldiers, officers, supervision
4. **Community**: Students, migrants
5. **Hybrid**: Workers with guards/supervisors

### Article Search Categories (Pack 3) - New!
1. **Refugees_Communities**: Community and refugee situations
2. **Phones_Chinese_Forums**: Phone seizures and forum discussions
3. **Locating_Workers_Region**: Geographic worker location searches
4. **Abuse_Exploitation_Asylum**: Abuse and exploitation cases
5. **Groups_to_Check**: Specific organizations and groups
6. **Hiring_DPRK_Workers**: Employment and hiring practices
7. **Corporate_Warehouse_Cases**: Corporate and warehouse investigations

## Privacy & Ethics

This system is designed for:
- Human rights documentation
- Academic research
- Journalistic investigation
- NGO reporting

**NOT** for:
- Commercial exploitation
- Privacy violation
- Harmful surveillance
- Discriminatory profiling

## Troubleshooting Commands

### System Diagnostics
```bash
# Full system check
./run.sh basic

# Check Python environment
uv pip list | grep -E "sqlalchemy|playwright|ollama|PIL"

# Verify environment variables
grep -E "SERP_API|DB_NAME|OLLAMA" .env

# Test database connection
uv run --no-project python -c "from database.connection import get_session; s=get_session(); print('âœ“ DB OK')"
```

### Ollama Issues
```bash
# Check if running
curl -s http://localhost:11434/api/tags || echo "Ollama not running"

# Start Ollama
ollama serve &

# Restart Ollama
pkill ollama && sleep 2 && ollama serve &

# Check for llava model
ollama list | grep llava || ollama pull llava

# Test image analysis capability
echo "Testing Ollama..." | ollama run llava "What do you see?"
```

### Database Issues
```bash
# Check PostgreSQL status
pg_ctl status || brew services list | grep postgresql

# Start PostgreSQL
brew services start postgresql

# Restart PostgreSQL
brew services restart postgresql

# Reset database completely
dropdb dprk 2>/dev/null; createdb dprk
uv run --no-project python init_database.py

# Check table counts
psql -U postgres -d dprk -c "\dt"
```

### API/Network Issues
```bash
# Test SERP API
uv run --no-project python -c "from search.serp_image_client import SerpImageClient; c=SerpImageClient(); print('âœ“ SERP OK' if c.api_key else 'âœ— No API key')"

# Test network connectivity
curl -I https://google.com

# Check rate limit status
grep RATE_LIMIT .env

# Reset failed downloads
psql -U postgres -d dprk -c "UPDATE search_results SET image_download_status='pending' WHERE image_download_status='failed';"
```

### Storage Issues
```bash
# Check disk space
df -h captured_data/

# Find large files
find captured_data/ -type f -size +10M -ls

# Clean temporary files
find captured_data/ -name "*.tmp" -delete

# Archive old data
tar -czf captured_data_$(date +%Y%m%d).tar.gz captured_data/
```

### Common Error Fixes
```bash
# "Module not found" error
uv pip install -r requirements.txt

# "Playwright browser not found"
uv run playwright install chromium

# "Database does not exist"
createdb dprk
uv run --no-project python init_database.py

# "Ollama model not found"
ollama pull llava

# "Permission denied" errors
chmod +x run.sh setup.sh
```

## Performance

### Image Pipeline
- Processes ~104 image search queries
- Downloads 10-20 images per query
- Analyzes 5 images per query with LLM
- Total runtime: ~30-60 minutes for full dataset

### Article Pipeline (New!)
- Processes 58 article search queries across 7 categories
- Collects ~50 results per query (~2,900 URLs total)
- Scrapes content with 50 concurrent Firecrawl requests
- Analyzes articles with Gemma3:12b for DPRK relevance
- Optimized for high-throughput processing

## Storage

- Images: ~50-100MB per session
- Screenshots: ~20-50MB per session
- Database: ~10MB for metadata

## Contributing

This is a sensitive research project. Please ensure:
1. Respect privacy and human rights
2. Follow ethical guidelines
3. Document findings responsibly
4. Protect sensitive data

## License

For research and human rights documentation only.

## Support

For issues or questions:
- Check DESIGN_DOCUMENT.md for technical details
- Review CHANGELOG.md for recent changes
- Consult logs in the `logs/` directory

## Disclaimer

This system is for legitimate research and human rights documentation only. Users are responsible for compliance with applicable laws and ethical guidelines.